{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smolagents in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (4.48.1)\n",
      "Requirement already satisfied: requests>=2.32.3 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (2.32.3)\n",
      "Requirement already satisfied: rich>=13.9.4 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (13.9.4)\n",
      "Requirement already satisfied: pandas>=2.2.3 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (2.2.3)\n",
      "Requirement already satisfied: jinja2>=3.1.4 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (3.1.5)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (11.1.0)\n",
      "Requirement already satisfied: markdownify>=0.14.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (0.14.1)\n",
      "Requirement already satisfied: gradio>=5.8.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (5.12.0)\n",
      "Requirement already satisfied: duckduckgo-search>=6.3.7 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (7.2.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (1.0.1)\n",
      "Requirement already satisfied: e2b-code-interpreter>=1.0.3 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (1.0.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from smolagents) (0.20.1)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from duckduckgo-search>=6.3.7->smolagents) (8.1.8)\n",
      "Requirement already satisfied: primp>=0.10.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from duckduckgo-search>=6.3.7->smolagents) (0.10.1)\n",
      "Requirement already satisfied: lxml>=5.3.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from duckduckgo-search>=6.3.7->smolagents) (5.3.0)\n",
      "Requirement already satisfied: attrs>=21.3.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from e2b-code-interpreter>=1.0.3->smolagents) (24.3.0)\n",
      "Requirement already satisfied: e2b<2.0.0,>=1.0.4 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from e2b-code-interpreter>=1.0.3->smolagents) (1.0.5)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.20.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from e2b-code-interpreter>=1.0.3->smolagents) (0.28.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (4.8.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.115.6)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.5.4 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (1.5.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.27.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.2.2)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (3.10.15)\n",
      "Requirement already satisfied: packaging in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.10.5)\n",
      "Requirement already satisfied: pydub in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.9.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.41.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.34.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio-client==1.5.4->gradio>=5.8.0->smolagents) (2024.12.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from gradio-client==1.5.4->gradio>=5.8.0->smolagents) (14.2)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from markdownify>=0.14.1->smolagents) (4.12.3)\n",
      "Requirement already satisfied: six<2,>=1.15 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from markdownify>=0.14.1->smolagents) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from pandas>=2.2.3->smolagents) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from pandas>=2.2.3->smolagents) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from pandas>=2.2.3->smolagents) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests>=2.32.3->smolagents) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests>=2.32.3->smolagents) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests>=2.32.3->smolagents) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests>=2.32.3->smolagents) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from rich>=13.9.4->smolagents) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from rich>=13.9.4->smolagents) (2.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from transformers>=4.0.0->smolagents) (3.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from transformers>=4.0.0->smolagents) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from transformers>=4.0.0->smolagents) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from transformers>=4.0.0->smolagents) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from transformers>=4.0.0->smolagents) (4.67.1)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from torchvision->smolagents) (2.5.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from torch==2.5.1->torchvision->smolagents) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from torch==2.5.1->torchvision->smolagents) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from torch==2.5.1->torchvision->smolagents) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision->smolagents) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio>=5.8.0->smolagents) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from beautifulsoup4<5,>=4.9->markdownify>=0.14.1->smolagents) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from click>=8.1.7->duckduckgo-search>=6.3.7->smolagents) (0.4.6)\n",
      "Requirement already satisfied: httpcore<2.0.0,>=1.0.5 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from e2b<2.0.0,>=1.0.4->e2b-code-interpreter>=1.0.3->smolagents) (1.0.7)\n",
      "Requirement already satisfied: protobuf<6.0.0,>=3.20.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from e2b<2.0.0,>=1.0.4->e2b-code-interpreter>=1.0.3->smolagents) (5.29.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from httpcore<2.0.0,>=1.0.5->e2b<2.0.0,>=1.0.4->e2b-code-interpreter>=1.0.3->smolagents) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from pydantic>=2.0->gradio>=5.8.0->smolagents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from pydantic>=2.0->gradio>=5.8.0->smolagents) (2.27.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio>=5.8.0->smolagents) (1.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\AI-Agents\\1-smolagent-summarizer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from smolagents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages that are used in our tools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_hugging_face_top_daily_paper() -> str:\n",
    "    \"\"\"\n",
    "    Get the top daily paper from Hugging Face\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://huggingface.co/papers\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # raise an exception in case of failure (4xx or 5xx status code)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the title element from the JSON-like data in the \"data-props\" attribute of the div element\n",
    "        containers = soup.find_all(\"div\", class_ = \"SVELTE_HYDRATER contents\")\n",
    "        top_paper = \"\"\n",
    "        for container in containers:\n",
    "            data_props = container.get(\"data-props\", '')\n",
    "            if data_props:\n",
    "                try:\n",
    "                    # Parse the JSON-like string\n",
    "                    json_data = json.loads(data_props.replace('&quot;', '\"'))\n",
    "                    if 'dailyPapers' in json_data:\n",
    "                        top_paper = json_data['dailyPapers'][0]['title']\n",
    "                except json.JSONDecodeError:\n",
    "                        continue\n",
    "        return top_paper\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occured while fetching the HTML: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the arxiv paper id by its title.\n",
    "    It returns the title of the paper\n",
    "\n",
    "    Args:\n",
    "    title: The title of the paper for which we want to get the ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        papers = api.list_papers(query=title)\n",
    "        if papers:\n",
    "            paper = next(iter(papers))\n",
    "            return paper.id\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured while fetching the paper ID: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the paper using its ID is done using the python arxiv package. Save the paper locally to use it for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "# install arxiv package\n",
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "@tool\n",
    "def download_paper_by_id(paper_id:str) -> None:\n",
    "    \"\"\"\n",
    "    This tool gets the id of a paper and downloads it from arxiv. \n",
    "    It saves the paper locally in the current directory as \"paper.pdf\"\n",
    "\n",
    "    Args:\n",
    "    paper_id: The arxiv id of the paper to download\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "        paper.download_pdf(filename=\"paper.pdf\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured while downloading the paper: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the pypdf package to read the PDF file. Read only the first three pages of the paper to save on token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\raj\\repos\\ai-agents\\1-smolagent-summarizer\\.venv\\lib\\site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool reads the content of a PDF file and returns it as a string\n",
    "\n",
    "    Args:\n",
    "    file_path: The path to the PDF file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = \"\"\n",
    "        reader = PdfReader('paper.pdf')\n",
    "        print(f\"Number of pages in the PDF file: {len(reader.pages)}\")\n",
    "        # Read the first 3 pages to save on tokens\n",
    "        for page in reader.pages[:3]:\n",
    "            content += page.extract_text()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured while reading the PDF file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Summarize today's top paper from Hugging Face daily papers by reading it.</span>                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mSummarize today's top paper from Hugging Face daily papers by reading it.\u001b[0m                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_hugging_face_top_daily_paper()</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Top paper title:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title)</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_paper_id_by_title(title</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title)</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Paper ID:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, paper_id)</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">download_paper_by_id(paper_id</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id)</span><span style=\"background-color: #272822\">                                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_content </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> read_pdf_file(file_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"paper.pdf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(paper_content)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_hugging_face_top_daily_paper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_paper_id_by_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper ID:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdownload_paper_by_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mread_pdf_file\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_path\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mpaper.pdf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the PDF file: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Top paper title: SRMT: Shared Memory for Multi-agent Lifelong Pathfinding\n",
       "Paper ID: 2501.13200\n",
       "Preprint\n",
       "SRMT: S HARED MEMORY FOR MULTI -AGENT LIFE -\n",
       "LONG PATHFINDING\n",
       "Alsu Sagirova1,2 Yuri Kuratov1,2 Mikhail Burtsev3\n",
       "1AIRI, Moscow, Russia\n",
       "2Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia\n",
       "3London Institute for Mathematical Sciences, London, UK\n",
       "{alsu.sagirova, yurii.kuratov}@phystech.edu, mb@lims.ac.uk\n",
       "ABSTRACT\n",
       "Multi-agent reinforcement learning (MARL) demonstrates significant progress in\n",
       "solving cooperative and competitive multi-agent problems in various environ-\n",
       "ments. One of the principal challenges in MARL is the need for explicit pre-\n",
       "diction of the agents’ behavior to achieve cooperation. To resolve this issue,\n",
       "we propose the Shared Recurrent Memory Transformer (SRMT) which extends\n",
       "memory transformers to multi-agent settings by pooling and globally broadcast-\n",
       "ing individual working memories, enabling agents to exchange information im-\n",
       "plicitly and coordinate their actions. We evaluate SRMT on the Partially Observ-\n",
       "able Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that\n",
       "requires agents to pass through a narrow corridor and on a POGEMA bench-\n",
       "mark set of tasks. In the Bottleneck task, SRMT consistently outperforms a va-\n",
       "riety of reinforcement learning baselines, especially under sparse rewards, and\n",
       "generalizes effectively to longer corridors than those seen during training. On\n",
       "POGEMA maps, including Mazes, Random, and MovingAI, SRMT is com-\n",
       "petitive with recent MARL, hybrid, and planning-based algorithms. These re-\n",
       "sults suggest that incorporating shared recurrent memory into the transformer-\n",
       "based architectures can enhance coordination in decentralized multi-agent sys-\n",
       "tems. The source code for training and evaluation is available on GitHub:\n",
       "https://github.com/Aloriosa/srmt.\n",
       "1 I NTRODUCTION\n",
       "Multi-agent systems have significant potential to solve complex problems through distributed intel-\n",
       "ligence and collaboration. However, coordinating the interactions between multiple agents remains\n",
       "challenging, often requiring sophisticated communication protocols and decision-making mecha-\n",
       "nisms. We propose a novel approach to address this challenge by introducing a shared memory as\n",
       "a global workspace for agents to coordinate behavior. The global workspace theory (Baars, 1988)\n",
       "suggests that in the brain, there are independent functional modules that can cooperate by broad-\n",
       "casting information through a global workspace. Inspired by this theory, we consider the agents\n",
       "in Multi-Agent Pathfinding (MAPF) as independent modules with shared memory and propose a\n",
       "Shared Recurrent Memory Transformer(SRMT) as a mechanism for exchanging information to im-\n",
       "prove coordination and avoid deadlocks. SRMT extends memory transformers (Burtsev et al., 2020;\n",
       "Bulatov et al., 2022; Yang et al., 2022; Cherepanov et al., 2024) to multi-agent settings by pooling\n",
       "and globally broadcasting individual working memories.\n",
       "In this study, we test the shared recurrent memory approach on Partially Observable Multi-agent\n",
       "Pathfinding (PO-MAPF) (Stern et al., 2019), where each agent aims to reach its goal while observing\n",
       "the state of the environment, including locations and actions of the other agents and static obstacles,\n",
       "only locally. Multi-agent pathfinding can be considered in the decentralized manner, where each\n",
       "agent independently collects rewards and makes decisions on its actions. There are many attempts\n",
       "to solve this problem: in robotics (Van den Berg et al., 2008; Zhu et al., 2022), in machine and\n",
       "reinforcement learning field (Damani et al., 2021; Ma et al., 2021b; Wang et al., 2023; Sartoretti\n",
       "et al., 2019; Riviere et al., 2020). Such methods often involve manual reward-shaping and external\n",
       "1\n",
       "arXiv:2501.13200v1  [cs.LG]  22 Jan 2025Preprint\n",
       "demonstrations. Also, several works utilize the communication between agents to solve decentral-\n",
       "ized MAPF (Ma et al., 2021a; Li et al., 2022; Wang et al., 2023). However, the resulting solutions\n",
       "are prone to deadlocks and poorly generalizable to the maps not used for training. In this work, we\n",
       "compare SRMT to a range of reinforcement learning baselines and show that it consistently out-\n",
       "performs them in the Bottleneck navigation task. Tests on POGEMA benchmark Skrynnik et al.\n",
       "(2024a) show that SRMT is competitive with numerous recent MARL, hybrid, and planning-based\n",
       "algorithms.\n",
       "2 R ELATED WORK\n",
       "2.1 S HARED MEMORY AND COMMUNICATION IN MULTI -AGENT REINFORCEMENT LEARNING\n",
       "Shared memory is designed to help agents coordinate their behavior, and it is closely related to ex-\n",
       "isting approaches in multi-agent reinforcement learning (MARL), particularly those involving inter-\n",
       "agent communication. Compared to a single-agent reinforcement learning, providing agents with\n",
       "communication protocol presents a significant challenge in MARL (Foerster et al., 2016; Iqbal &amp;\n",
       "Sha, 2018; Zhang et al., 2018). Common strategies to address this problem include (1) a centralized\n",
       "setting, where a central controller aggregates information from all agents; (2) a fully decentralized\n",
       "setting, where agents make decisions based solely on local observations; and (3) a decentralized\n",
       "setting with networked agents, allowing agents to share local information with each other (Zhang\n",
       "et al., 2021; Hu et al., 2023; Nayak et al., 2023; Agarwal et al., 2019).\n",
       "In multi-agent pathfinding (MAPF) with reinforcement learning, various methods fit within these\n",
       "three categories. Decentralized methods without communication include approaches such as\n",
       "IQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2020), QPLEX (Wang et al.,\n",
       "2021), Follower (Skrynnik et al., 2024b), and MATS-LP (Skrynnik et al., 2024c). These meth-\n",
       "ods propose to implement the agent decision-making based only on local information. Notably,\n",
       "VDN, QMIX, and QPLEX adopt centralized training with a joint Q-function but operate in a de-\n",
       "centralized manner during execution with individual Q-functions. In contrast, centralized methods\n",
       "such as LaCAM (Okumura, 2023) and RHCR (Li et al., 2021) employ a centralized search-based\n",
       "planner. Finally, decentralized methods with communication, such as DCC (Ma et al., 2021b),\n",
       "MAMBA (Egorov &amp; Shpilman, 2022), and SCRIMP (Wang et al., 2023), allow agents to share\n",
       "information to enhance coordination and avoid collisions. These methods utilize various commu-\n",
       "nication strategies, including selective information sharing (DCC), discrete communication pro-\n",
       "tocols (MAMBA), and scalable communication mechanisms based on transformer architectures\n",
       "(SCRIMP), all aimed at improving agent cooperation in complex and dynamic environments.\n",
       "MAMBA (Egorov &amp; Shpilman, 2022) is a pure MARL approach that uses communication and\n",
       "centralized training within a Model-Based Reinforcement Learning framework, featuring discrete\n",
       "communication and decentralized execution. A 3-layer transformer serves as the communication\n",
       "block with its outputs used by the agents to update their world models and make action predic-\n",
       "tions. Each agent maintains its own version of the world model, which can be updated through the\n",
       "communication block.\n",
       "QPLEX (Wang et al., 2021) is a pure multi-agent reinforcement learning method that employs\n",
       "multi-agent Q-learning with centralized end-to-end training, providing inter-agent communication.\n",
       "QPLEX learns to factorize a joint action-value function to enable decentralized execution. The\n",
       "approach uses a duplex dueling mechanism that connects local and joint action-value functions,\n",
       "allowing agents to make independent decisions while benefiting from centralized training.\n",
       "Follower (Skrynnik et al., 2024b) is a learnable method for lifelong MAPF without communication\n",
       "that uses a centralized path planning with a modified A* heuristic search to reduce agents’ colli-\n",
       "sions. MATS-LP (Skrynnik et al., 2024c) is also a learnable method for lifelong MAPF without\n",
       "communication that uses a learnable policy combined with Monte Carlo Tree Search (MCTS) to\n",
       "reason about possible future states.\n",
       "RHCR (Li et al., 2021) is a purely search-based centralized planner that does not require training.\n",
       "It decomposes the lifelong MAPF into a sequence of windowed MAPF instances, with re-planning\n",
       "occurring according to a predetermined schedule, resolving collisions within the current planning\n",
       "window only.\n",
       "2Preprint\n",
       "The proposed shared recurrent memory approach fits into a decentralized setting with networked\n",
       "agents. Unlike other methods, SRMT allows agents to communicate indirectly through a shared\n",
       "memory. Each agent uses a recurrently updated memory and learns to read and write its individ-\n",
       "ual memory representations to a shared space. This allows agents to retain information over time\n",
       "steps in the episode and enables the effective individual and collective decision-making process in\n",
       "pathfinding tasks. Furthermore, unlike approaches that rely on a joint Q-function for centralized\n",
       "training, our method maintains decentralization throughout training and execution. The fully de-\n",
       "centralized setting of the multi-agent system might be preferable over the centralized one in many\n",
       "real-world applications.\n",
       "2.2 S HARED MEMORY AND MEMORY TRANSFORMERS\n",
       "SRMT extends the memory transformers (Burtsev et al., 2020; Bulatov et al., 2022; Yang et al., 2022;\n",
       "Cherepanov et al., 2024) to multi-agent settings by pooling and globally broadcasting individual\n",
       "memories of each agent.\n",
       "Memory Transformer (Burtsev et al., 2020) augments the standard Transformer architec-\n",
       "ture (Vaswani et al., 2017) by introducing special memory tokens appended to the input sequence,\n",
       "providing the additional operational space for the model. These memory tokens are trainable and\n",
       "are used by the model as working memory. In the Recurrent Memory Transformer (RMT) (Bulatov\n",
       "et al., 2022), memory tokens transfer information between segments of a long input sequence. In\n",
       "this case, multiple memory tokens act as a recurrent state, effectively turning the transformer into a\n",
       "recurrent cell that processes each segment as input at each time step.\n",
       "Agent Transformer Memory (ATM) (Yang et al., 2022) introduces a transformer-based working\n",
       "memory into multi-agent reinforcement learning. Each ATM agent maintains a memory buffer of\n",
       "the last N memory states, sequentially updated by a transformer network. This approach is similar\n",
       "to the RMT, but instead of using only the latest memory state, ATM uses the several most recent\n",
       "memory states. Additionally, each memory state in ATM consists of a single vector, whereas in\n",
       "RMT, multiple memory vectors work together as a recurrent hidden state.\n",
       "Recurrent Action Transformer with Memory (RATE) (Cherepanov et al., 2024) extends the De-\n",
       "cision Transformer (Chen et al., 2021) by incorporating memory tokens and a dedicated memory\n",
       "update module, the Memory Retention Valve, which updates memory states to effectively handle\n",
       "long segmented trajectories.\n",
       "Relational Recurrent Neural Network (RRNN) (Santoro et al., 2018) utilizes the multi-head dot\n",
       "product attention to update the memory state given the new input data. Then it employs the modifi-\n",
       "cation of a standard LSTM (Hochreiter, 1997) treating the memory matrix as a matrix of recurrent\n",
       "cell states to predict the model outputs.\n",
       "Approaches such as ATM, RATE, and RRNN are focused on maintaining individual memory states\n",
       "for each agent. In contrast, SRMT extends recurrent memory to multi-agent RL and facilitates\n",
       "the shared access to individual agents’ memories, enabling coordination and joint decision-making\n",
       "among all agents in the environment.\n",
       "3 S HARED RECURRENT MEMORY TRANSFORMER\n",
       "Multi-agent pathfinding task is set as follows. The agents interact in the two-dimensional environ-\n",
       "ment represented as graph G = (V, E) with the vertices corresponding to the locations and the\n",
       "edges to the transitions between these locations. The timeline consists of discrete time steps. The\n",
       "predefined final step of the agent interaction episode is called the episode length. At the beginning\n",
       "of the episode, each agent i is given a start location si ∈ V and a goal location gi ∈ V to be reached\n",
       "until the end of the episode. At each time step, an agent performs an action to stay in its current\n",
       "location or move to an adjacent one. The task of multi-agent pathfinding is to make each agent reach\n",
       "its goal until the end of the episode without colliding with the other agents.\n",
       "In this study, we work with a decentralized Partially Observable Multi-agent Pathfinding (PO-\n",
       "MAPF) (Stern et al., 2019). Decentralization of MAPF means that the multi-agent system has\n",
       "no global controller, each agent performs actions and collects rewards independently of the oth-\n",
       "ers. Also, each agent observes the environment obstacles, other agents, and their goals only locally,\n",
       "3\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Top paper title: SRMT: Shared Memory for Multi-agent Lifelong Pathfinding\n",
       "Paper ID: 2501.13200\n",
       "Preprint\n",
       "SRMT: S HARED MEMORY FOR MULTI -AGENT LIFE -\n",
       "LONG PATHFINDING\n",
       "Alsu Sagirova1,2 Yuri Kuratov1,2 Mikhail Burtsev3\n",
       "1AIRI, Moscow, Russia\n",
       "2Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia\n",
       "3London Institute for Mathematical Sciences, London, UK\n",
       "{alsu.sagirova, yurii.kuratov}@phystech.edu, mb@lims.ac.uk\n",
       "ABSTRACT\n",
       "Multi-agent reinforcement learning (MARL) demonstrates significant progress in\n",
       "solving cooperative and competitive multi-agent problems in various environ-\n",
       "ments. One of the principal challenges in MARL is the need for explicit pre-\n",
       "diction of the agents’ behavior to achieve cooperation. To resolve this issue,\n",
       "we propose the Shared Recurrent Memory Transformer (SRMT) which extends\n",
       "memory transformers to multi-agent settings by pooling and globally broadcast-\n",
       "ing individual working memories, enabling agents to exchange information im-\n",
       "plicitly and coordinate their actions. We evaluate SRMT on the Partially Observ-\n",
       "able Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that\n",
       "requires agents to pass through a narrow corridor and on a POGEMA bench-\n",
       "mark set of tasks. In the Bottleneck task, SRMT consistently outperforms a va-\n",
       "riety of reinforcement learning baselines, especially under sparse rewards, and\n",
       "generalizes effectively to longer corridors than those seen during training. On\n",
       "POGEMA maps, including Mazes, Random, and MovingAI, SRMT is com-\n",
       "petitive with recent MARL, hybrid, and planning-based algorithms. These re-\n",
       "sults suggest that incorporating shared recurrent memory into the transformer-\n",
       "based architectures can enhance coordination in decentralized multi-agent sys-\n",
       "tems. The source code for training and evaluation is available on GitHub:\n",
       "https://github.com/Aloriosa/srmt.\n",
       "1 I NTRODUCTION\n",
       "Multi-agent systems have significant potential to solve complex problems through distributed intel-\n",
       "ligence and collaboration. However, coordinating the interactions between multiple agents remains\n",
       "challenging, often requiring sophisticated communication protocols and decision-making mecha-\n",
       "nisms. We propose a novel approach to address this challenge by introducing a shared memory as\n",
       "a global workspace for agents to coordinate behavior. The global workspace theory (Baars, 1988)\n",
       "suggests that in the brain, there are independent functional modules that can cooperate by broad-\n",
       "casting information through a global workspace. Inspired by this theory, we consider the agents\n",
       "in Multi-Agent Pathfinding (MAPF) as independent modules with shared memory and propose a\n",
       "Shared Recurrent Memory Transformer(SRMT) as a mechanism for exchanging information to im-\n",
       "prove coordination and avoid deadlocks. SRMT extends memory transformers (Burtsev et al., 2020;\n",
       "Bulatov et al., 2022; Yang et al., 2022; Cherepanov et al., 2024) to multi-agent settings by pooling\n",
       "and globally broadcasting individual working memories.\n",
       "In this study, we test the shared recurrent memory approach on Partially Observable Multi-agent\n",
       "Pathfinding (PO-MAPF) (Stern et al., 2019), where each agent aims to reach its goal while observing\n",
       "the state of the environment, including locations and actions of the other agents and static obstacles,\n",
       "only locally. Multi-agent pathfinding can be considered in the decentralized manner, where each\n",
       "agent independently collects rewards and makes decisions on its actions. There are many attempts\n",
       "to solve this problem: in robotics (Van den Berg et al., 2008; Zhu et al., 2022), in machine and\n",
       "reinforcement learning field (Damani et al., 2021; Ma et al., 2021b; Wang et al., 2023; Sartoretti\n",
       "et al., 2019; Riviere et al., 2020). Such methods often involve manual reward-shaping and external\n",
       "1\n",
       "arXiv:2501.13200v1  [cs.LG]  22 Jan 2025Preprint\n",
       "demonstrations. Also, several works utilize the communication between agents to solve decentral-\n",
       "ized MAPF (Ma et al., 2021a; Li et al., 2022; Wang et al., 2023). However, the resulting solutions\n",
       "are prone to deadlocks and poorly generalizable to the maps not used for training. In this work, we\n",
       "compare SRMT to a range of reinforcement learning baselines and show that it consistently out-\n",
       "performs them in the Bottleneck navigation task. Tests on POGEMA benchmark Skrynnik et al.\n",
       "(2024a) show that SRMT is competitive with numerous recent MARL, hybrid, and planning-based\n",
       "algorithms.\n",
       "2 R ELATED WORK\n",
       "2.1 S HARED MEMORY AND COMMUNICATION IN MULTI -AGENT REINFORCEMENT LEARNING\n",
       "Shared memory is designed to help agents coordinate their behavior, and it is closely related to ex-\n",
       "isting approaches in multi-agent reinforcement learning (MARL), particularly those involving inter-\n",
       "agent communication. Compared to a single-agent reinforcement learning, providing agents with\n",
       "communication protocol presents a significant challenge in MARL (Foerster et al., 2016; Iqbal &\n",
       "Sha, 2018; Zhang et al., 2018). Common strategies to address this problem include (1) a centralized\n",
       "setting, where a central controller aggregates information from all agents; (2) a fully decentralized\n",
       "setting, where agents make decisions based solely on local observations; and (3) a decentralized\n",
       "setting with networked agents, allowing agents to share local information with each other (Zhang\n",
       "et al., 2021; Hu et al., 2023; Nayak et al., 2023; Agarwal et al., 2019).\n",
       "In multi-agent pathfinding (MAPF) with reinforcement learning, various methods fit within these\n",
       "three categories. Decentralized methods without communication include approaches such as\n",
       "IQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2020), QPLEX (Wang et al.,\n",
       "2021), Follower (Skrynnik et al., 2024b), and MATS-LP (Skrynnik et al., 2024c). These meth-\n",
       "ods propose to implement the agent decision-making based only on local information. Notably,\n",
       "VDN, QMIX, and QPLEX adopt centralized training with a joint Q-function but operate in a de-\n",
       "centralized manner during execution with individual Q-functions. In contrast, centralized methods\n",
       "such as LaCAM (Okumura, 2023) and RHCR (Li et al., 2021) employ a centralized search-based\n",
       "planner. Finally, decentralized methods with communication, such as DCC (Ma et al., 2021b),\n",
       "MAMBA (Egorov & Shpilman, 2022), and SCRIMP (Wang et al., 2023), allow agents to share\n",
       "information to enhance coordination and avoid collisions. These methods utilize various commu-\n",
       "nication strategies, including selective information sharing (DCC), discrete communication pro-\n",
       "tocols (MAMBA), and scalable communication mechanisms based on transformer architectures\n",
       "(SCRIMP), all aimed at improving agent cooperation in complex and dynamic environments.\n",
       "MAMBA (Egorov & Shpilman, 2022) is a pure MARL approach that uses communication and\n",
       "centralized training within a Model-Based Reinforcement Learning framework, featuring discrete\n",
       "communication and decentralized execution. A 3-layer transformer serves as the communication\n",
       "block with its outputs used by the agents to update their world models and make action predic-\n",
       "tions. Each agent maintains its own version of the world model, which can be updated through the\n",
       "communication block.\n",
       "QPLEX (Wang et al., 2021) is a pure multi-agent reinforcement learning method that employs\n",
       "multi-agent Q-learning with centralized end-to-end training, providing inter-agent communication.\n",
       "QPLEX learns to factorize a joint action-value function to enable decentralized execution. The\n",
       "approach uses a duplex dueling mechanism that connects local and joint action-value functions,\n",
       "allowing agents to make independent decisions while benefiting from centralized training.\n",
       "Follower (Skrynnik et al., 2024b) is a learnable method for lifelong MAPF without communication\n",
       "that uses a centralized path planning with a modified A* heuristic search to reduce agents’ colli-\n",
       "sions. MATS-LP (Skrynnik et al., 2024c) is also a learnable method for lifelong MAPF without\n",
       "communication that uses a learnable policy combined with Monte Carlo Tree Search (MCTS) to\n",
       "reason about possible future states.\n",
       "RHCR (Li et al., 2021) is a purely search-based centralized planner that does not require training.\n",
       "It decomposes the lifelong MAPF into a sequence of windowed MAPF instances, with re-planning\n",
       "occurring according to a predetermined schedule, resolving collisions within the current planning\n",
       "window only.\n",
       "2Preprint\n",
       "The proposed shared recurrent memory approach fits into a decentralized setting with networked\n",
       "agents. Unlike other methods, SRMT allows agents to communicate indirectly through a shared\n",
       "memory. Each agent uses a recurrently updated memory and learns to read and write its individ-\n",
       "ual memory representations to a shared space. This allows agents to retain information over time\n",
       "steps in the episode and enables the effective individual and collective decision-making process in\n",
       "pathfinding tasks. Furthermore, unlike approaches that rely on a joint Q-function for centralized\n",
       "training, our method maintains decentralization throughout training and execution. The fully de-\n",
       "centralized setting of the multi-agent system might be preferable over the centralized one in many\n",
       "real-world applications.\n",
       "2.2 S HARED MEMORY AND MEMORY TRANSFORMERS\n",
       "SRMT extends the memory transformers (Burtsev et al., 2020; Bulatov et al., 2022; Yang et al., 2022;\n",
       "Cherepanov et al., 2024) to multi-agent settings by pooling and globally broadcasting individual\n",
       "memories of each agent.\n",
       "Memory Transformer (Burtsev et al., 2020) augments the standard Transformer architec-\n",
       "ture (Vaswani et al., 2017) by introducing special memory tokens appended to the input sequence,\n",
       "providing the additional operational space for the model. These memory tokens are trainable and\n",
       "are used by the model as working memory. In the Recurrent Memory Transformer (RMT) (Bulatov\n",
       "et al., 2022), memory tokens transfer information between segments of a long input sequence. In\n",
       "this case, multiple memory tokens act as a recurrent state, effectively turning the transformer into a\n",
       "recurrent cell that processes each segment as input at each time step.\n",
       "Agent Transformer Memory (ATM) (Yang et al., 2022) introduces a transformer-based working\n",
       "memory into multi-agent reinforcement learning. Each ATM agent maintains a memory buffer of\n",
       "the last N memory states, sequentially updated by a transformer network. This approach is similar\n",
       "to the RMT, but instead of using only the latest memory state, ATM uses the several most recent\n",
       "memory states. Additionally, each memory state in ATM consists of a single vector, whereas in\n",
       "RMT, multiple memory vectors work together as a recurrent hidden state.\n",
       "Recurrent Action Transformer with Memory (RATE) (Cherepanov et al., 2024) extends the De-\n",
       "cision Transformer (Chen et al., 2021) by incorporating memory tokens and a dedicated memory\n",
       "update module, the Memory Retention Valve, which updates memory states to effectively handle\n",
       "long segmented trajectories.\n",
       "Relational Recurrent Neural Network (RRNN) (Santoro et al., 2018) utilizes the multi-head dot\n",
       "product attention to update the memory state given the new input data. Then it employs the modifi-\n",
       "cation of a standard LSTM (Hochreiter, 1997) treating the memory matrix as a matrix of recurrent\n",
       "cell states to predict the model outputs.\n",
       "Approaches such as ATM, RATE, and RRNN are focused on maintaining individual memory states\n",
       "for each agent. In contrast, SRMT extends recurrent memory to multi-agent RL and facilitates\n",
       "the shared access to individual agents’ memories, enabling coordination and joint decision-making\n",
       "among all agents in the environment.\n",
       "3 S HARED RECURRENT MEMORY TRANSFORMER\n",
       "Multi-agent pathfinding task is set as follows. The agents interact in the two-dimensional environ-\n",
       "ment represented as graph G = (V, E) with the vertices corresponding to the locations and the\n",
       "edges to the transitions between these locations. The timeline consists of discrete time steps. The\n",
       "predefined final step of the agent interaction episode is called the episode length. At the beginning\n",
       "of the episode, each agent i is given a start location si ∈ V and a goal location gi ∈ V to be reached\n",
       "until the end of the episode. At each time step, an agent performs an action to stay in its current\n",
       "location or move to an adjacent one. The task of multi-agent pathfinding is to make each agent reach\n",
       "its goal until the end of the episode without colliding with the other agents.\n",
       "In this study, we work with a decentralized Partially Observable Multi-agent Pathfinding (PO-\n",
       "MAPF) (Stern et al., 2019). Decentralization of MAPF means that the multi-agent system has\n",
       "no global controller, each agent performs actions and collects rewards independently of the oth-\n",
       "ers. Also, each agent observes the environment obstacles, other agents, and their goals only locally,\n",
       "3\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 5.99 seconds| Input tokens: 2,379 | Output tokens: 127]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 0: Duration 5.99 seconds| Input tokens: 2,379 | Output tokens: 127]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Extracting the main components of the paper</span><span style=\"background-color: #272822\">                                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">abstract </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> paper_content</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"1 I NTRODUCTION\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"ABSTRACT\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">introduction </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> paper_content</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"1 I NTRODUCTION\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"2 R ELATED WORK\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">related_work </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> paper_content</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"2 R ELATED WORK\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"3 S HARED RECURRENT MEMORY </span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">TRANSFORMER\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">                                                                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">srmt_description </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> paper_content</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"3 S HARED RECURRENT MEMORY TRANSFORMER\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">split(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"4\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">strip()</span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Summarizing the key points</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">summary </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Abstract: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">abstract</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Introduction: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">introduction</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Related Work: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">related_work</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">SRMT </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Description: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">srmt_description</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(summary)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Extracting the main components of the paper\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mabstract\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m1 I NTRODUCTION\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mABSTRACT\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mintroduction\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m1 I NTRODUCTION\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m2 R ELATED WORK\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mrelated_work\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m2 R ELATED WORK\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m3 S HARED RECURRENT MEMORY \u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mTRANSFORMER\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msrmt_description\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m3 S HARED RECURRENT MEMORY TRANSFORMER\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m4\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstrip\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Summarizing the key points\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mAbstract: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mabstract\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mIntroduction: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mintroduction\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mRelated Work: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrelated_work\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mSRMT \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mDescription: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msrmt_description\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: Abstract: Multi-agent reinforcement learning (MARL) demonstrates significant progress in</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">solving cooperative and competitive multi-agent problems in various environ-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ments. One of the principal challenges in MARL is the need for explicit pre-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">diction of the agents’ behavior to achieve cooperation. To resolve this issue,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">we propose the Shared Recurrent Memory Transformer (SRMT) which extends</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory transformers to multi-agent settings by pooling and globally broadcast-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ing individual working memories, enabling agents to exchange information im-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">plicitly and coordinate their actions. We evaluate SRMT on the Partially Observ-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">able Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">requires agents to pass through a narrow corridor and on a POGEMA bench-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">mark set of tasks. In the Bottleneck task, SRMT consistently outperforms a va-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">riety of reinforcement learning baselines, especially under sparse rewards, and</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">generalizes effectively to longer corridors than those seen during training. On</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">POGEMA maps, including Mazes, Random, and MovingAI, SRMT is com-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">petitive with recent MARL, hybrid, and planning-based algorithms. These re-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">sults suggest that incorporating shared recurrent memory into the transformer-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">based architectures can enhance coordination in decentralized multi-agent sys-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">tems. The source code for training and evaluation is available on GitHub:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">https://github.com/Aloriosa/srmt.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Introduction: Multi-agent systems have significant potential to solve complex problems through distributed intel-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ligence and collaboration. However, coordinating the interactions between multiple agents remains</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">challenging, often requiring sophisticated communication protocols and decision-making mecha-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">nisms. We propose a novel approach to address this challenge by introducing a shared memory as</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">a global workspace for agents to coordinate behavior. The global workspace theory (Baars, 1988)</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">suggests that in the brain, there are independent functional modules that can cooperate by broad-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">casting information through a global workspace. Inspired by this theory, we consider the agents</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">in Multi-Agent Pathfinding (MAPF) as independent modules with shared memory and propose a</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Shared Recurrent Memory Transformer(SRMT) as a mechanism for exchanging information to im-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">prove coordination and avoid deadlocks. SRMT extends memory transformers (Burtsev et al., 2020;</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Bulatov et al., 2022; Yang et al., 2022; Cherepanov et al., 2024) to multi-agent settings by pooling</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">and globally broadcasting individual working memories.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">In this study, we test the shared recurrent memory approach on Partially Observable Multi-agent</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Pathfinding (PO-MAPF) (Stern et al., 2019), where each agent aims to reach its goal while observing</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the state of the environment, including locations and actions of the other agents and static obstacles,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">only locally. Multi-agent pathfinding can be considered in the decentralized manner, where each</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">agent independently collects rewards and makes decisions on its actions. There are many attempts</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">to solve this problem: in robotics (Van den Berg et al., 2008; Zhu et al., 2022), in machine and</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">reinforcement learning field (Damani et al., 2021; Ma et al., 2021b; Wang et al., 2023; Sartoretti</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">et al., 2019; Riviere et al., 2020). Such methods often involve manual reward-shaping and external</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">arXiv:2501.13200v1  [cs.LG]  22 Jan 2025Preprint</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">demonstrations. Also, several works utilize the communication between agents to solve decentral-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ized MAPF (Ma et al., 2021a; Li et al., 2022; Wang et al., 2023). However, the resulting solutions</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">are prone to deadlocks and poorly generalizable to the maps not used for training. In this work, we</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">compare SRMT to a range of reinforcement learning baselines and show that it consistently out-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">performs them in the Bottleneck navigation task. Tests on POGEMA benchmark Skrynnik et al.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">(2024a) show that SRMT is competitive with numerous recent MARL, hybrid, and planning-based</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">algorithms.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Related Work: 2.1 S HARED MEMORY AND COMMUNICATION IN MULTI -AGENT REINFORCEMENT LEARNING</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Shared memory is designed to help agents coordinate their behavior, and it is closely related to ex-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">isting approaches in multi-agent reinforcement learning (MARL), particularly those involving inter-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">agent communication. Compared to a single-agent reinforcement learning, providing agents with</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">communication protocol presents a significant challenge in MARL (Foerster et al., 2016; Iqbal &amp;</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Sha, 2018; Zhang et al., 2018). Common strategies to address this problem include (1) a centralized</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">setting, where a central controller aggregates information from all agents; (2) a fully decentralized</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">setting, where agents make decisions based solely on local observations; and (3) a decentralized</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">setting with networked agents, allowing agents to share local information with each other (Zhang</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">et al., 2021; Hu et al., 2023; Nayak et al., 2023; Agarwal et al., 2019).</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">In multi-agent pathfinding (MAPF) with reinforcement learning, various methods fit within these</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">three categories. Decentralized methods without communication include approaches such as</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">IQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2020), QPLEX (Wang et al.,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2021), Follower (Skrynnik et al., 2024b), and MATS-LP (Skrynnik et al., 2024c). These meth-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ods propose to implement the agent decision-making based only on local information. Notably,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">VDN, QMIX, and QPLEX adopt centralized training with a joint Q-function but operate in a de-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">centralized manner during execution with individual Q-functions. In contrast, centralized methods</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">such as LaCAM (Okumura, 2023) and RHCR (Li et al., 2021) employ a centralized search-based</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">planner. Finally, decentralized methods with communication, such as DCC (Ma et al., 2021b),</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">MAMBA (Egorov &amp; Shpilman, 2022), and SCRIMP (Wang et al., 2023), allow agents to share</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">information to enhance coordination and avoid collisions. These methods utilize various commu-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">nication strategies, including selective information sharing (DCC), discrete communication pro-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">tocols (MAMBA), and scalable communication mechanisms based on transformer architectures</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">(SCRIMP), all aimed at improving agent cooperation in complex and dynamic environments.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">MAMBA (Egorov &amp; Shpilman, 2022) is a pure MARL approach that uses communication and</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">centralized training within a Model-Based Reinforcement Learning framework, featuring discrete</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">communication and decentralized execution. A 3-layer transformer serves as the communication</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">block with its outputs used by the agents to update their world models and make action predic-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">tions. Each agent maintains its own version of the world model, which can be updated through the</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">communication block.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">QPLEX (Wang et al., 2021) is a pure multi-agent reinforcement learning method that employs</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">multi-agent Q-learning with centralized end-to-end training, providing inter-agent communication.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">QPLEX learns to factorize a joint action-value function to enable decentralized execution. The</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">approach uses a duplex dueling mechanism that connects local and joint action-value functions,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">allowing agents to make independent decisions while benefiting from centralized training.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Follower (Skrynnik et al., 2024b) is a learnable method for lifelong MAPF without communication</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">that uses a centralized path planning with a modified A* heuristic search to reduce agents’ colli-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">sions. MATS-LP (Skrynnik et al., 2024c) is also a learnable method for lifelong MAPF without</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">communication that uses a learnable policy combined with Monte Carlo Tree Search (MCTS) to</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">reason about possible future states.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">RHCR (Li et al., 2021) is a purely search-based centralized planner that does not require training.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">It decomposes the lifelong MAPF into a sequence of windowed MAPF instances, with re-planning</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">occurring according to a predetermined schedule, resolving collisions within the current planning</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">window only.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2Preprint</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">The proposed shared recurrent memory approach fits into a decentralized setting with networked</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">agents. Unlike other methods, SRMT allows agents to communicate indirectly through a shared</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory. Each agent uses a recurrently updated memory and learns to read and write its individ-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ual memory representations to a shared space. This allows agents to retain information over time</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">steps in the episode and enables the effective individual and collective decision-making process in</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">pathfinding tasks. Furthermore, unlike approaches that rely on a joint Q-function for centralized</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">training, our method maintains decentralization throughout training and execution. The fully de-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">centralized setting of the multi-agent system might be preferable over the centralized one in many</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">real-world applications.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2.2 S HARED MEMORY AND MEMORY TRANSFORMERS</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">SRMT extends the memory transformers (Burtsev et al., 2020; Bulatov et al., 2022; Yang et al., 2022;</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Cherepanov et al., 2024) to multi-agent settings by pooling and globally broadcasting individual</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memories of each agent.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Memory Transformer (Burtsev et al., 2020) augments the standard Transformer architec-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ture (Vaswani et al., 2017) by introducing special memory tokens appended to the input sequence,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">providing the additional operational space for the model. These memory tokens are trainable and</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">are used by the model as working memory. In the Recurrent Memory Transformer (RMT) (Bulatov</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">et al., 2022), memory tokens transfer information between segments of a long input sequence. In</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">this case, multiple memory tokens act as a recurrent state, effectively turning the transformer into a</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">recurrent cell that processes each segment as input at each time step.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Agent Transformer Memory (ATM) (Yang et al., 2022) introduces a transformer-based working</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory into multi-agent reinforcement learning. Each ATM agent maintains a memory buffer of</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the last N memory states, sequentially updated by a transformer network. This approach is similar</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">to the RMT, but instead of using only the latest memory state, ATM uses the several most recent</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory states. Additionally, each memory state in ATM consists of a single vector, whereas in</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">RMT, multiple memory vectors work together as a recurrent hidden state.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Recurrent Action Transformer with Memory (RATE) (Cherepanov et al., 2024) extends the De-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">cision Transformer (Chen et al., 2021) by incorporating memory tokens and a dedicated memory</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">update module, the Memory Retention Valve, which updates memory states to effectively handle</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">long segmented trajectories.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Relational Recurrent Neural Network (RRNN) (Santoro et al., 2018) utilizes the multi-head dot</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">product attention to update the memory state given the new input data. Then it employs the modifi-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">cation of a standard LSTM (Hochreiter, 1997) treating the memory matrix as a matrix of recurrent</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">cell states to predict the model outputs.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Approaches such as ATM, RATE, and RRNN are focused on maintaining individual memory states</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">for each agent. In contrast, SRMT extends recurrent memory to multi-agent RL and facilitates</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the shared access to individual agents’ memories, enabling coordination and joint decision-making</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">among all agents in the environment.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">SRMT Description: Multi-agent pathfinding task is set as follows. The agents interact in the two-dimensional </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">environ-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ment represented as graph G = (V, E) with the vertices corresponding to the locations and the</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">edges to the transitions between these locations. The timeline consists of discrete time steps. The</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">predefined final step of the agent interaction episode is called the episode length. At the beginning</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">of the episode, each agent i is given a start location si ∈ V and a goal location gi ∈ V to be reached</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">until the end of the episode. At each time step, an agent performs an action to stay in its current</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">location or move to an adjacent one. The task of multi-agent pathfinding is to make each agent reach</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">its goal until the end of the episode without colliding with the other agents.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">In this study, we work with a decentralized Partially Observable Multi-agent Pathfinding (PO-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">MAPF) (Stern et al., 2019). Decentralization of MAPF means that the multi-agent system has</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">no global controller, each agent performs actions and collects rewards independently of the oth-</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">ers. Also, each agent observes the environment obstacles, other agents, and their goals only locally,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: Abstract: Multi-agent reinforcement learning (MARL) demonstrates significant progress in\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msolving cooperative and competitive multi-agent problems in various environ-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mments. One of the principal challenges in MARL is the need for explicit pre-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdiction of the agents’ behavior to achieve cooperation. To resolve this issue,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mwe propose the Shared Recurrent Memory Transformer (SRMT) which extends\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemory transformers to multi-agent settings by pooling and globally broadcast-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2ming individual working memories, enabling agents to exchange information im-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mplicitly and coordinate their actions. We evaluate SRMT on the Partially Observ-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mrequires agents to pass through a narrow corridor and on a POGEMA bench-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a va-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mriety of reinforcement learning baselines, especially under sparse rewards, and\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mgeneralizes effectively to longer corridors than those seen during training. On\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mPOGEMA maps, including Mazes, Random, and MovingAI, SRMT is com-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpetitive with recent MARL, hybrid, and planning-based algorithms. These re-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msults suggest that incorporating shared recurrent memory into the transformer-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mbased architectures can enhance coordination in decentralized multi-agent sys-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtems. The source code for training and evaluation is available on GitHub:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mhttps://github.com/Aloriosa/srmt.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mIntroduction: Multi-agent systems have significant potential to solve complex problems through distributed intel-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mligence and collaboration. However, coordinating the interactions between multiple agents remains\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mchallenging, often requiring sophisticated communication protocols and decision-making mecha-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mnisms. We propose a novel approach to address this challenge by introducing a shared memory as\u001b[0m\n",
       "\u001b[1;38;2;212;183;2ma global workspace for agents to coordinate behavior. The global workspace theory (Baars, 1988)\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msuggests that in the brain, there are independent functional modules that can cooperate by broad-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcasting information through a global workspace. Inspired by this theory, we consider the agents\u001b[0m\n",
       "\u001b[1;38;2;212;183;2min Multi-Agent Pathfinding (MAPF) as independent modules with shared memory and propose a\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mShared Recurrent Memory Transformer(SRMT) as a mechanism for exchanging information to im-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mprove coordination and avoid deadlocks. SRMT extends memory transformers (Burtsev et al., 2020;\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mBulatov et al., 2022; Yang et al., 2022; Cherepanov et al., 2024) to multi-agent settings by pooling\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mand globally broadcasting individual working memories.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mIn this study, we test the shared recurrent memory approach on Partially Observable Multi-agent\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mPathfinding (PO-MAPF) (Stern et al., 2019), where each agent aims to reach its goal while observing\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe state of the environment, including locations and actions of the other agents and static obstacles,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2monly locally. Multi-agent pathfinding can be considered in the decentralized manner, where each\u001b[0m\n",
       "\u001b[1;38;2;212;183;2magent independently collects rewards and makes decisions on its actions. There are many attempts\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mto solve this problem: in robotics (Van den Berg et al., 2008; Zhu et al., 2022), in machine and\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mreinforcement learning field (Damani et al., 2021; Ma et al., 2021b; Wang et al., 2023; Sartoretti\u001b[0m\n",
       "\u001b[1;38;2;212;183;2met al., 2019; Riviere et al., 2020). Such methods often involve manual reward-shaping and external\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m1\u001b[0m\n",
       "\u001b[1;38;2;212;183;2marXiv:2501.13200v1  [cs.LG]  22 Jan 2025Preprint\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdemonstrations. Also, several works utilize the communication between agents to solve decentral-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mized MAPF (Ma et al., 2021a; Li et al., 2022; Wang et al., 2023). However, the resulting solutions\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mare prone to deadlocks and poorly generalizable to the maps not used for training. In this work, we\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcompare SRMT to a range of reinforcement learning baselines and show that it consistently out-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mperforms them in the Bottleneck navigation task. Tests on POGEMA benchmark Skrynnik et al.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m(2024a) show that SRMT is competitive with numerous recent MARL, hybrid, and planning-based\u001b[0m\n",
       "\u001b[1;38;2;212;183;2malgorithms.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mRelated Work: 2.1 S HARED MEMORY AND COMMUNICATION IN MULTI -AGENT REINFORCEMENT LEARNING\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mShared memory is designed to help agents coordinate their behavior, and it is closely related to ex-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2misting approaches in multi-agent reinforcement learning (MARL), particularly those involving inter-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2magent communication. Compared to a single-agent reinforcement learning, providing agents with\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcommunication protocol presents a significant challenge in MARL (Foerster et al., 2016; Iqbal &\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mSha, 2018; Zhang et al., 2018). Common strategies to address this problem include (1) a centralized\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msetting, where a central controller aggregates information from all agents; (2) a fully decentralized\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msetting, where agents make decisions based solely on local observations; and (3) a decentralized\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msetting with networked agents, allowing agents to share local information with each other (Zhang\u001b[0m\n",
       "\u001b[1;38;2;212;183;2met al., 2021; Hu et al., 2023; Nayak et al., 2023; Agarwal et al., 2019).\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mIn multi-agent pathfinding (MAPF) with reinforcement learning, various methods fit within these\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthree categories. Decentralized methods without communication include approaches such as\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mIQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2020), QPLEX (Wang et al.,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2021), Follower (Skrynnik et al., 2024b), and MATS-LP (Skrynnik et al., 2024c). These meth-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mods propose to implement the agent decision-making based only on local information. Notably,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mVDN, QMIX, and QPLEX adopt centralized training with a joint Q-function but operate in a de-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcentralized manner during execution with individual Q-functions. In contrast, centralized methods\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msuch as LaCAM (Okumura, 2023) and RHCR (Li et al., 2021) employ a centralized search-based\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mplanner. Finally, decentralized methods with communication, such as DCC (Ma et al., 2021b),\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mMAMBA (Egorov & Shpilman, 2022), and SCRIMP (Wang et al., 2023), allow agents to share\u001b[0m\n",
       "\u001b[1;38;2;212;183;2minformation to enhance coordination and avoid collisions. These methods utilize various commu-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mnication strategies, including selective information sharing (DCC), discrete communication pro-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtocols (MAMBA), and scalable communication mechanisms based on transformer architectures\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m(SCRIMP), all aimed at improving agent cooperation in complex and dynamic environments.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mMAMBA (Egorov & Shpilman, 2022) is a pure MARL approach that uses communication and\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcentralized training within a Model-Based Reinforcement Learning framework, featuring discrete\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcommunication and decentralized execution. A 3-layer transformer serves as the communication\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mblock with its outputs used by the agents to update their world models and make action predic-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtions. Each agent maintains its own version of the world model, which can be updated through the\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcommunication block.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mQPLEX (Wang et al., 2021) is a pure multi-agent reinforcement learning method that employs\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmulti-agent Q-learning with centralized end-to-end training, providing inter-agent communication.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mQPLEX learns to factorize a joint action-value function to enable decentralized execution. The\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mapproach uses a duplex dueling mechanism that connects local and joint action-value functions,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mallowing agents to make independent decisions while benefiting from centralized training.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mFollower (Skrynnik et al., 2024b) is a learnable method for lifelong MAPF without communication\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthat uses a centralized path planning with a modified A* heuristic search to reduce agents’ colli-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msions. MATS-LP (Skrynnik et al., 2024c) is also a learnable method for lifelong MAPF without\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcommunication that uses a learnable policy combined with Monte Carlo Tree Search (MCTS) to\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mreason about possible future states.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mRHCR (Li et al., 2021) is a purely search-based centralized planner that does not require training.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mIt decomposes the lifelong MAPF into a sequence of windowed MAPF instances, with re-planning\u001b[0m\n",
       "\u001b[1;38;2;212;183;2moccurring according to a predetermined schedule, resolving collisions within the current planning\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mwindow only.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2Preprint\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mThe proposed shared recurrent memory approach fits into a decentralized setting with networked\u001b[0m\n",
       "\u001b[1;38;2;212;183;2magents. Unlike other methods, SRMT allows agents to communicate indirectly through a shared\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemory. Each agent uses a recurrently updated memory and learns to read and write its individ-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mual memory representations to a shared space. This allows agents to retain information over time\u001b[0m\n",
       "\u001b[1;38;2;212;183;2msteps in the episode and enables the effective individual and collective decision-making process in\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpathfinding tasks. Furthermore, unlike approaches that rely on a joint Q-function for centralized\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtraining, our method maintains decentralization throughout training and execution. The fully de-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcentralized setting of the multi-agent system might be preferable over the centralized one in many\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mreal-world applications.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2.2 S HARED MEMORY AND MEMORY TRANSFORMERS\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mSRMT extends the memory transformers (Burtsev et al., 2020; Bulatov et al., 2022; Yang et al., 2022;\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mCherepanov et al., 2024) to multi-agent settings by pooling and globally broadcasting individual\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemories of each agent.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mMemory Transformer (Burtsev et al., 2020) augments the standard Transformer architec-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mture (Vaswani et al., 2017) by introducing special memory tokens appended to the input sequence,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mproviding the additional operational space for the model. These memory tokens are trainable and\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mare used by the model as working memory. In the Recurrent Memory Transformer (RMT) (Bulatov\u001b[0m\n",
       "\u001b[1;38;2;212;183;2met al., 2022), memory tokens transfer information between segments of a long input sequence. In\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthis case, multiple memory tokens act as a recurrent state, effectively turning the transformer into a\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mrecurrent cell that processes each segment as input at each time step.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mAgent Transformer Memory (ATM) (Yang et al., 2022) introduces a transformer-based working\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemory into multi-agent reinforcement learning. Each ATM agent maintains a memory buffer of\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe last N memory states, sequentially updated by a transformer network. This approach is similar\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mto the RMT, but instead of using only the latest memory state, ATM uses the several most recent\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemory states. Additionally, each memory state in ATM consists of a single vector, whereas in\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mRMT, multiple memory vectors work together as a recurrent hidden state.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mRecurrent Action Transformer with Memory (RATE) (Cherepanov et al., 2024) extends the De-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcision Transformer (Chen et al., 2021) by incorporating memory tokens and a dedicated memory\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mupdate module, the Memory Retention Valve, which updates memory states to effectively handle\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlong segmented trajectories.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mRelational Recurrent Neural Network (RRNN) (Santoro et al., 2018) utilizes the multi-head dot\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mproduct attention to update the memory state given the new input data. Then it employs the modifi-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcation of a standard LSTM (Hochreiter, 1997) treating the memory matrix as a matrix of recurrent\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcell states to predict the model outputs.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mApproaches such as ATM, RATE, and RRNN are focused on maintaining individual memory states\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfor each agent. In contrast, SRMT extends recurrent memory to multi-agent RL and facilitates\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe shared access to individual agents’ memories, enabling coordination and joint decision-making\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mamong all agents in the environment.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mSRMT Description: Multi-agent pathfinding task is set as follows. The agents interact in the two-dimensional \u001b[0m\n",
       "\u001b[1;38;2;212;183;2menviron-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mment represented as graph G = (V, E) with the vertices corresponding to the locations and the\u001b[0m\n",
       "\u001b[1;38;2;212;183;2medges to the transitions between these locations. The timeline consists of discrete time steps. The\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpredefined final step of the agent interaction episode is called the episode length. At the beginning\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mof the episode, each agent i is given a start location si ∈ V and a goal location gi ∈ V to be reached\u001b[0m\n",
       "\u001b[1;38;2;212;183;2muntil the end of the episode. At each time step, an agent performs an action to stay in its current\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlocation or move to an adjacent one. The task of multi-agent pathfinding is to make each agent reach\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mits goal until the end of the episode without colliding with the other agents.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mIn this study, we work with a decentralized Partially Observable Multi-agent Pathfinding (PO-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mMAPF) (Stern et al., 2019). Decentralization of MAPF means that the multi-agent system has\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mno global controller, each agent performs actions and collects rewards independently of the oth-\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mers. Also, each agent observes the environment obstacles, other agents, and their goals only locally,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 5.19 seconds| Input tokens: 8,005 | Output tokens: 327]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 5.19 seconds| Input tokens: 8,005 | Output tokens: 327]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Abstract: Multi-agent reinforcement learning (MARL) demonstrates significant progress in\\nsolving cooperative and competitive multi-agent problems in various environ-\\nments. One of the principal challenges in MARL is the need for explicit pre-\\ndiction of the agents’ behavior to achieve cooperation. To resolve this issue,\\nwe propose the Shared Recurrent Memory Transformer (SRMT) which extends\\nmemory transformers to multi-agent settings by pooling and globally broadcast-\\ning individual working memories, enabling agents to exchange information im-\\nplicitly and coordinate their actions. We evaluate SRMT on the Partially Observ-\\nable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that\\nrequires agents to pass through a narrow corridor and on a POGEMA bench-\\nmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a va-\\nriety of reinforcement learning baselines, especially under sparse rewards, and\\ngeneralizes effectively to longer corridors than those seen during training. On\\nPOGEMA maps, including Mazes, Random, and MovingAI, SRMT is com-\\npetitive with recent MARL, hybrid, and planning-based algorithms. These re-\\nsults suggest that incorporating shared recurrent memory into the transformer-\\nbased architectures can enhance coordination in decentralized multi-agent sys-\\ntems. The source code for training and evaluation is available on GitHub:\\nhttps://github.com/Aloriosa/srmt.\\n\\nIntroduction: Multi-agent systems have significant potential to solve complex problems through distributed intel-\\nligence and collaboration. However, coordinating the interactions between multiple agents remains\\nchallenging, often requiring sophisticated communication protocols and decision-making mecha-\\nnisms. We propose a novel approach to address this challenge by introducing a shared memory as\\na global workspace for agents to coordinate behavior. The global workspace theory (Baars, 1988)\\nsuggests that in the brain, there are independent functional modules that can cooperate by broad-\\ncasting information through a global workspace. Inspired by this theory, we consider the agents\\nin Multi-Agent Pathfinding (MAPF) as independent modules with shared memory and propose a\\nShared Recurrent Memory Transformer(SRMT) as a mechanism for exchanging information to im-\\nprove coordination and avoid deadlocks. SRMT extends memory transformers (Burtsev et al., 2020;\\nBulatov et al., 2022; Yang et al., 2022; Cherepanov et al., 2024) to multi-agent settings by pooling\\nand globally broadcasting individual working memories.\\nIn this study, we test the shared recurrent memory approach on Partially Observable Multi-agent\\nPathfinding (PO-MAPF) (Stern et al., 2019), where each agent aims to reach its goal while observing\\nthe state of the environment, including locations and actions of the other agents and static obstacles,\\nonly locally. Multi-agent pathfinding can be considered in the decentralized manner, where each\\nagent independently collects rewards and makes decisions on its actions. There are many attempts\\nto solve this problem: in robotics (Van den Berg et al., 2008; Zhu et al., 2022), in machine and\\nreinforcement learning field (Damani et al., 2021; Ma et al., 2021b; Wang et al., 2023; Sartoretti\\net al., 2019; Riviere et al., 2020). Such methods often involve manual reward-shaping and external\\n1\\narXiv:2501.13200v1  [cs.LG]  22 Jan 2025Preprint\\ndemonstrations. Also, several works utilize the communication between agents to solve decentral-\\nized MAPF (Ma et al., 2021a; Li et al., 2022; Wang et al., 2023). However, the resulting solutions\\nare prone to deadlocks and poorly generalizable to the maps not used for training. In this work, we\\ncompare SRMT to a range of reinforcement learning baselines and show that it consistently out-\\nperforms them in the Bottleneck navigation task. Tests on POGEMA benchmark Skrynnik et al.\\n(2024a) show that SRMT is competitive with numerous recent MARL, hybrid, and planning-based\\nalgorithms.\\n\\nRelated Work: 2.1 S HARED MEMORY AND COMMUNICATION IN MULTI -AGENT REINFORCEMENT LEARNING\\nShared memory is designed to help agents coordinate their behavior, and it is closely related to ex-\\nisting approaches in multi-agent reinforcement learning (MARL), particularly those involving inter-\\nagent communication. Compared to a single-agent reinforcement learning, providing agents with\\ncommunication protocol presents a significant challenge in MARL (Foerster et al., 2016; Iqbal &\\nSha, 2018; Zhang et al., 2018). Common strategies to address this problem include (1) a centralized\\nsetting, where a central controller aggregates information from all agents; (2) a fully decentralized\\nsetting, where agents make decisions based solely on local observations; and (3) a decentralized\\nsetting with networked agents, allowing agents to share local information with each other (Zhang\\net al., 2021; Hu et al., 2023; Nayak et al., 2023; Agarwal et al., 2019).\\nIn multi-agent pathfinding (MAPF) with reinforcement learning, various methods fit within these\\nthree categories. Decentralized methods without communication include approaches such as\\nIQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2020), QPLEX (Wang et al.,\\n2021), Follower (Skrynnik et al., 2024b), and MATS-LP (Skrynnik et al., 2024c). These meth-\\nods propose to implement the agent decision-making based only on local information. Notably,\\nVDN, QMIX, and QPLEX adopt centralized training with a joint Q-function but operate in a de-\\ncentralized manner during execution with individual Q-functions. In contrast, centralized methods\\nsuch as LaCAM (Okumura, 2023) and RHCR (Li et al., 2021) employ a centralized search-based\\nplanner. Finally, decentralized methods with communication, such as DCC (Ma et al., 2021b),\\nMAMBA (Egorov & Shpilman, 2022), and SCRIMP (Wang et al., 2023), allow agents to share\\ninformation to enhance coordination and avoid collisions. These methods utilize various commu-\\nnication strategies, including selective information sharing (DCC), discrete communication pro-\\ntocols (MAMBA), and scalable communication mechanisms based on transformer architectures\\n(SCRIMP), all aimed at improving agent cooperation in complex and dynamic environments.\\nMAMBA (Egorov & Shpilman, 2022) is a pure MARL approach that uses communication and\\ncentralized training within a Model-Based Reinforcement Learning framework, featuring discrete\\ncommunication and decentralized execution. A 3-layer transformer serves as the communication\\nblock with its outputs used by the agents to update their world models and make action predic-\\ntions. Each agent maintains its own version of the world model, which can be updated through the\\ncommunication block.\\nQPLEX (Wang et al., 2021) is a pure multi-agent reinforcement learning method that employs\\nmulti-agent Q-learning with centralized end-to-end training, providing inter-agent communication.\\nQPLEX learns to factorize a joint action-value function to enable decentralized execution. The\\napproach uses a duplex dueling mechanism that connects local and joint action-value functions,\\nallowing agents to make independent decisions while benefiting from centralized training.\\nFollower (Skrynnik et al., 2024b) is a learnable method for lifelong MAPF without communication\\nthat uses a centralized path planning with a modified A* heuristic search to reduce agents’ colli-\\nsions. MATS-LP (Skrynnik et al., 2024c) is also a learnable method for lifelong MAPF without\\ncommunication that uses a learnable policy combined with Monte Carlo Tree Search (MCTS) to\\nreason about possible future states.\\nRHCR (Li et al., 2021) is a purely search-based centralized planner that does not require training.\\nIt decomposes the lifelong MAPF into a sequence of windowed MAPF instances, with re-planning\\noccurring according to a predetermined schedule, resolving collisions within the current planning\\nwindow only.\\n2Preprint\\nThe proposed shared recurrent memory approach fits into a decentralized setting with networked\\nagents. Unlike other methods, SRMT allows agents to communicate indirectly through a shared\\nmemory. Each agent uses a recurrently updated memory and learns to read and write its individ-\\nual memory representations to a shared space. This allows agents to retain information over time\\nsteps in the episode and enables the effective individual and collective decision-making process in\\npathfinding tasks. Furthermore, unlike approaches that rely on a joint Q-function for centralized\\ntraining, our method maintains decentralization throughout training and execution. The fully de-\\ncentralized setting of the multi-agent system might be preferable over the centralized one in many\\nreal-world applications.\\n2.2 S HARED MEMORY AND MEMORY TRANSFORMERS\\nSRMT extends the memory transformers (Burtsev et al., 2020; Bulatov et al., 2022; Yang et al., 2022;\\nCherepanov et al., 2024) to multi-agent settings by pooling and globally broadcasting individual\\nmemories of each agent.\\nMemory Transformer (Burtsev et al., 2020) augments the standard Transformer architec-\\nture (Vaswani et al., 2017) by introducing special memory tokens appended to the input sequence,\\nproviding the additional operational space for the model. These memory tokens are trainable and\\nare used by the model as working memory. In the Recurrent Memory Transformer (RMT) (Bulatov\\net al., 2022), memory tokens transfer information between segments of a long input sequence. In\\nthis case, multiple memory tokens act as a recurrent state, effectively turning the transformer into a\\nrecurrent cell that processes each segment as input at each time step.\\nAgent Transformer Memory (ATM) (Yang et al., 2022) introduces a transformer-based working\\nmemory into multi-agent reinforcement learning. Each ATM agent maintains a memory buffer of\\nthe last N memory states, sequentially updated by a transformer network. This approach is similar\\nto the RMT, but instead of using only the latest memory state, ATM uses the several most recent\\nmemory states. Additionally, each memory state in ATM consists of a single vector, whereas in\\nRMT, multiple memory vectors work together as a recurrent hidden state.\\nRecurrent Action Transformer with Memory (RATE) (Cherepanov et al., 2024) extends the De-\\ncision Transformer (Chen et al., 2021) by incorporating memory tokens and a dedicated memory\\nupdate module, the Memory Retention Valve, which updates memory states to effectively handle\\nlong segmented trajectories.\\nRelational Recurrent Neural Network (RRNN) (Santoro et al., 2018) utilizes the multi-head dot\\nproduct attention to update the memory state given the new input data. Then it employs the modifi-\\ncation of a standard LSTM (Hochreiter, 1997) treating the memory matrix as a matrix of recurrent\\ncell states to predict the model outputs.\\nApproaches such as ATM, RATE, and RRNN are focused on maintaining individual memory states\\nfor each agent. In contrast, SRMT extends recurrent memory to multi-agent RL and facilitates\\nthe shared access to individual agents’ memories, enabling coordination and joint decision-making\\namong all agents in the environment.\\n\\nSRMT Description: Multi-agent pathfinding task is set as follows. The agents interact in the two-dimensional environ-\\nment represented as graph G = (V, E) with the vertices corresponding to the locations and the\\nedges to the transitions between these locations. The timeline consists of discrete time steps. The\\npredefined final step of the agent interaction episode is called the episode length. At the beginning\\nof the episode, each agent i is given a start location si ∈ V and a goal location gi ∈ V to be reached\\nuntil the end of the episode. At each time step, an agent performs an action to stay in its current\\nlocation or move to an adjacent one. The task of multi-agent pathfinding is to make each agent reach\\nits goal until the end of the episode without colliding with the other agents.\\nIn this study, we work with a decentralized Partially Observable Multi-agent Pathfinding (PO-\\nMAPF) (Stern et al., 2019). Decentralization of MAPF means that the multi-agent system has\\nno global controller, each agent performs actions and collects rewards independently of the oth-\\ners. Also, each agent observes the environment obstacles, other agents, and their goals only locally,\\n3'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smolagents import CodeAgent, HfApiModel\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "\n",
    "model = HfApiModel(model_id = model_id, token=os.getenv(\"HF_TOKEN\"))\n",
    "agent = CodeAgent(tools=[get_hugging_face_top_daily_paper, \n",
    "                         get_paper_id_by_title, \n",
    "                         download_paper_by_id, \n",
    "                         read_pdf_file], \n",
    "                         model=model,\n",
    "                         add_base_tools=True)\n",
    "agent.run(\"Summarize today's top paper from Hugging Face daily papers by reading it.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
